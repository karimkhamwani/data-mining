frame = data.frame(Policy_Status = factor(c("Uninsured","Insured"), levels=c("Uninsured","Insured")), Count = c(Uninsured,Insured))
plot_ly(frame, x = ~Policy_Status, y = ~Count, type = "bar", color = frame$Policy_Status, colors = c("Purple", "Gold")) %>%
layout(title = "<b>Insuraned vs Uninsured<b>", legend = list(title = list(text ='<b> Policy Status </b>')))
# From the below graph we mapped those who were insured for Caravan insurance against those who were not.
# We see that the count of Uninsured = 9236 vs Insured = 586
# This led us to think about the factors why such a large amount of customers are not Caravan insured vs those who were
# Hence, our research question, to figure out the characteristics of those who have Caravan Insurance
ggplot(df,aes(factor(df$MOSTYPE))) +
geom_bar(aes(fill = factor(df$CARAVAN))) +
labs(x="Customer Sub type") +
scale_fill_discrete(name = "CARAVAN") +
ggtitle("Policy Bought based on Customer sub Type") +
theme(plot.title = element_text(hjust = 0.5))
df$subtype = df$MOSTYPE
nrow(df[df$subtype == 33 & df$CARAVAN == 1,])
nrow(df[df$subtype == 8 & df$CARAVAN == 1,])
# category 33 and 8 purchased more policies.
# Based on our main category which compromises of various sub-categories, we can see that sub-categories number 33 and 8 are porne to buying insurance. These should be considered as the characteristics/attributes of the types of customers that exist in the main customer category. hence, we could say that, those who are middle class and those who are low class but have large families have a higher chance of getting the insurance
ggplot(df,aes(factor(df$MAANTHUI))) +
geom_bar(aes(fill = factor(df$CARAVAN))) +
geom_text(stat='count', aes(label=..count..), vjust=0) +
labs(x="Number of houses") +
scale_fill_discrete(name = "CARAVAN") +
ggtitle("Number of houses customer has who bought insurance") +
theme(plot.title = element_text(hjust = 0.5))
df$noofhouses = df$MAANTHUI
nrow(df[df$noofhouses == 1 & df$CARAVAN == 1,])
nrow(df[df$noofhouses == 2 & df$CARAVAN == 1,])
nrow(df[df$noofhouses == 3 & df$CARAVAN == 1,])
# Now we wanted to see who is prone to getting an insurance with respect to number of houses and we have found that customers having at least 1 house are likely to get the insurance.
ggplot(df,aes(factor(df$MGEMOMV))) +
geom_bar(aes(fill = factor(df$CARAVAN))) +
geom_text(stat='count', aes(label=..count..), vjust=0) +
labs(x="Number of house hold") +
scale_fill_discrete(name = "CARAVAN") +
ggtitle("Number of house hold") +
theme(plot.title = element_text(hjust = 0.5))
df$noofhousehold = df$MGEMOMV
nrow(df[df$noofhousehold == 1 & df$CARAVAN == 1,])
nrow(df[df$noofhousehold == 2 & df$CARAVAN == 1,])
nrow(df[df$noofhousehold == 3 & df$CARAVAN == 1,])
df$hasThreeHouseHold = ifelse(df$noofhousehold == 3, 1, 0)
# Now we wanted to see who is prone to getting an insurance with respect to number of houses and we have found that customers having at least 1 house are likely to get the insurance.
corrplot(cor(df[, c("subtype","maintype", "age", "noofhouses", "noofhousehold", "CARAVAN")]), method = "number")
corrplot(cor(df[, c("subtype","maintype", "age", "noofhouses", "noofhousehold", "CARAVAN")]), method = "number")
ggplot(df,aes(factor(df$MGEMOMV))) +
geom_bar(aes(fill = factor(df$CARAVAN))) +
geom_text(stat='count', aes(label=..count..), vjust=0) +
labs(x="Number of house hold") +
scale_fill_discrete(name = "CARAVAN") +
ggtitle("Number of house hold") +
theme(plot.title = element_text(hjust = 0.5))
df$noofhousehold = df$MGEMOMV
nrow(df[df$noofhousehold == 1 & df$CARAVAN == 1,])
nrow(df[df$noofhousehold == 2 & df$CARAVAN == 1,])
nrow(df[df$noofhousehold == 3 & df$CARAVAN == 1,])
df$hasThreeHouseHold = ifelse(df$noofhousehold == 3, 1, 0)
# Now we wanted to see who is prone to getting an insurance with respect to number of houses and we have found that customers having at least 1 house are likely to get the insurance.
ggplot(df,aes(factor(df$MGEMLEEF))) +
geom_bar(aes(fill = factor(df$CARAVAN))) +
geom_text(stat='count', aes(label=..count..), vjust=0) +
labs(x="Age Group") +
scale_fill_discrete(name = "CARAVAN") +
ggtitle("Policy bought on age group") +
theme(plot.title = element_text(hjust = 0.5))
df$age = df$MGEMLEEF
nrow(df[df$age == 1 & df$CARAVAN == 1,])
nrow(df[df$age == 2 & df$CARAVAN == 1,])
nrow(df[df$age == 3 & df$CARAVAN == 1,])
nrow(df[df$age == 4 & df$CARAVAN == 1,])
nrow(df[df$age == 5 & df$CARAVAN == 1,])
nrow(df[df$age == 6 & df$CARAVAN == 1,])
# Here we have explored to see what is the age range of the customers that buy the insurance. Based on our analysis we see that customers who are between the ages 40-50 are prone to buying insurance compared with others. Hence, we could say that it is among the many characteristics of the main customer group [3,8]
ggplot(df,aes(factor(df$MAANTHUI))) +
geom_bar(aes(fill = factor(df$CARAVAN))) +
geom_text(stat='count', aes(label=..count..), vjust=0) +
labs(x="Number of houses") +
scale_fill_discrete(name = "CARAVAN") +
ggtitle("Number of houses customer has who bought insurance") +
theme(plot.title = element_text(hjust = 0.5))
df$noofhouses = df$MAANTHUI
nrow(df[df$noofhouses == 1 & df$CARAVAN == 1,])
nrow(df[df$noofhouses == 2 & df$CARAVAN == 1,])
nrow(df[df$noofhouses == 3 & df$CARAVAN == 1,])
# Now we wanted to see who is prone to getting an insurance with respect to number of houses and we have found that customers having at least 1 house are likely to get the insurance.
ggplot(df,aes(factor(df$MGEMOMV))) +
geom_bar(aes(fill = factor(df$CARAVAN))) +
geom_text(stat='count', aes(label=..count..), vjust=0) +
labs(x="Number of house hold") +
scale_fill_discrete(name = "CARAVAN") +
ggtitle("Number of house hold") +
theme(plot.title = element_text(hjust = 0.5))
df$noofhousehold = df$MGEMOMV
nrow(df[df$noofhousehold == 1 & df$CARAVAN == 1,])
nrow(df[df$noofhousehold == 2 & df$CARAVAN == 1,])
nrow(df[df$noofhousehold == 3 & df$CARAVAN == 1,])
df$hasThreeHouseHold = ifelse(df$noofhousehold == 3, 1, 0)
# Now we wanted to see who is prone to getting an insurance with respect to number of houses and we have found that customers having at least 1 house are likely to get the insurance.
corrplot(cor(df[, c("subtype","maintype", "age", "noofhouses", "noofhousehold", "CARAVAN")]), method = "number")
ggplot(df,aes(factor(df$MOSHOOFD))) +
geom_bar(aes(fill = factor(df$CARAVAN))) +
labs(x="Customer Main type") +
scale_fill_discrete(name = "CARAVAN") +
ggtitle("Caravan Policy based on Customer Main Type") +
theme(plot.title = element_text(hjust = 0.5))
df$maintype = df$MOSHOOFD
nrow(df[df$maintype == 1 & df$CARAVAN == 1,])
nrow(df[df$maintype == 2 & df$CARAVAN == 1,])
nrow(df[df$maintype == 3 & df$CARAVAN == 1,])
nrow(df[df$maintype == 4 & df$CARAVAN == 1,])
nrow(df[df$maintype == 5 & df$CARAVAN == 1,])
nrow(df[df$maintype == 6 & df$CARAVAN == 1,])
nrow(df[df$maintype == 7 & df$CARAVAN == 1,])
nrow(df[df$maintype == 8 & df$CARAVAN == 1,])
nrow(df[df$maintype == 9 & df$CARAVAN == 1,])
nrow(df[df$maintype == 10 & df$CARAVAN == 1,])
# Here we wanted to see the which customer main type has the highest frequency/count of buying the insurance. Based on results, we see that there are atleast 4 main customer categories that buy insurance. However, for our ease and understanding purposes we will only consider the top 2. This brings us to select, category number 8 and 3, where 8 = Family with grown ups and 3 = Driven Growths
corrplot(cor(df[, c("subtype","maintype", "age", "noofhouses", "noofhousehold", "CARAVAN")]), method = "number")
# From the correlation matrix we have below, we have some interesting insights. The reason to run the correlation matrix was to figure out variables of interest which might cause either over-fitting or under-fitting. # Here we can see that the variables of interest are positively correlated. One exception to this is the correlation between age and number of households. We see that there is a negative correlation between it. Which actually makes sense because, the greater the age, the no of households will decrease.
cor(df$subtype, df$CARAVAN)
cor(df$maintype, df$CARAVAN)
# we will choose which have high corelation with response variable in this case both are weak correlated doesn't matter what we really choose
# we will choose maintype
table(df$MAANTHUI)
df$MAANTHUI = replace(df$MAANTHUI, df$MAANTHUI > 2, 2)
df$OneHouse = ifelse(df$MAANTHUI ==1, 1, 0)
df$moreThanTwoHouse = ifelse(df$MAANTHUI > 2, 1, 0)
# by looking frequencies of houses we categorized houses into dummies. As we already saw from the above bar graph that customers having at least 1 house are likely to buy insurance, we thought it would be a good idea to create dummies in a binary way instead of having many different ranges. That is why we have done this step.
df$averageFamily = ifelse(df$MOSTYPE %in% c(12,11,9,10,13), 1, 0)
df$loners = ifelse(df$MOSTYPE %in% c(17,15,18,16,19), 1, 0)
df$conservativeFamilies = ifelse(df$MOSTYPE %in% c(39,38), 1, 0)
df$crusingSeniors = ifelse(df$MOSTYPE %in% c(26,25,28,27), 1, 0)
df$drivenGrowers = ifelse(df$MOSTYPE %in% c(6,7,8), 1, 0)
df$grownups = ifelse(df$MOSTYPE %in% c(33,34,35,36,37), 1, 0)
df$framers = ifelse(df$MOSTYPE %in% c(40,41), 1, 0)
df$livingWell = ifelse(df$MOSTYPE %in% c(20,21,22,23,24), 1, 0)
df$retired = ifelse(df$MOSTYPE %in% c(29,30,31,32), 1, 0)
df$successful = ifelse(df$MOSTYPE %in% c(1,2,3,4,5), 1, 0)
dat <- data.frame(
Categorized_Customers = factor(c("averageFamily", "loners", "conservativeFamilies", "crusingSeniors", "drivenGrowers", "grownups", "framers", "livingWell", "retired", "successful"), levels=c("averageFamily", "loners", "conservativeFamilies", "crusingSeniors", "drivenGrowers", "grownups", "framers", "livingWell", "retired", "successful")),
Count = c( sum(df$averageFamily), sum(df$loners), sum(df$conservativeFamilies), sum(df$crusingSeniors), sum(df$drivenGrowers), sum(df$grownups), sum(df$framers), sum(df$livingWell), sum(df$retired), sum(df$successful) )
)
plot_ly(dat, x = ~Categorized_Customers, y = ~Count, type = 'bar', color = dat$Categorized_Customers) %>%
layout(title = "<b>Customer Types<b>", legend = list(title = list(text ='<b> Types </b>')))
# We have 10 customer main types and 41 customer sub-types. A leveled approach was to consider merging them into one bucket based on sub-category. Since one of our goal is to find the characteristics of customers, we believe that the customer sub-type are the characteristics. However, when we look at customer main type alone, we cannot figure out the characteristics of people in this group. Hence, we merged sub-categories with main categories.
# An advantage of doing this is that when we look at customer main type, we would automatically know what are the characteristics of this group - explained by the sub types within this group
# After having done so, we constructed a bar chart and observed the following:
#1. Grown-ups are the most popular
#2. Followed by Average Family
#3. Followed by Conservative Families
# Converting 30k income into value
df$MINKM30_c = ifelse(df$MINKM30 == 1, 0.05 * 30000, df$MINKM30)
df$MINKM30_c = ifelse(df$MINKM30_c == 2, 0.17 * 30000, df$MINKM30_c)
df$MINKM30_c = ifelse(df$MINKM30_c == 3, 0.3 * 30000, df$MINKM30_c)
df$MINKM30_c = ifelse(df$MINKM30_c == 4, 0.43 * 30000, df$MINKM30_c)
df$MINKM30_c = ifelse(df$MINKM30_c == 5, 0.56 * 30000, df$MINKM30_c)
df$MINKM30_c = ifelse(df$MINKM30_c == 6, 0.69 * 30000, df$MINKM30_c)
df$MINKM30_c = ifelse(df$MINKM30_c == 7, 0.82 * 30000, df$MINKM30_c)
df$MINKM30_c = ifelse(df$MINKM30_c == 8, 0.94 * 30000, df$MINKM30_c)
df$MINKM30_c = ifelse(df$MINKM30_c == 9, 1 * 30000, df$MINKM30_c)
# Converting 45k income into value
df$MINK3045_c = ifelse(df$MINK3045 == 1, 0.05 * 45000, df$MINK3045)
df$MINK3045_c = ifelse(df$MINK3045_c == 2, 0.17 * 45000, df$MINK3045_c)
df$MINK3045_c = ifelse(df$MINK3045_c == 3, 0.3 * 45000, df$MINK3045_c)
df$MINK3045_c = ifelse(df$MINK3045_c == 4, 0.43 * 45000, df$MINK3045_c)
df$MINK3045_c = ifelse(df$MINK3045_c == 5, 0.56 * 45000, df$MINK3045_c)
df$MINK3045_c = ifelse(df$MINK3045_c == 6, 0.69 * 45000, df$MINK3045_c)
df$MINK3045_c = ifelse(df$MINK3045_c == 7, 0.82 * 45000, df$MINK3045_c)
df$MINK3045_c = ifelse(df$MINK3045_c == 8, 0.94 * 45000, df$MINK3045_c)
df$MINK3045_c = ifelse(df$MINK3045_c == 9, 1 * 45000, df$MINK3045_c)
# Converting 70k income into value
df$MINK4575_c = ifelse(df$MINK4575 == 1, 0.05 * 75000, df$MINK4575)
df$MINK4575_c = ifelse(df$MINK4575_c == 2, 0.17 * 75000, df$MINK4575_c)
df$MINK4575_c = ifelse(df$MINK4575_c == 3, 0.3 * 75000, df$MINK4575_c)
df$MINK4575_c = ifelse(df$MINK4575_c == 4, 0.43 * 75000, df$MINK4575_c)
df$MINK4575_c = ifelse(df$MINK4575_c == 5, 0.56 * 75000, df$MINK4575_c)
df$MINK4575_c = ifelse(df$MINK4575_c == 6, 0.69 * 75000, df$MINK4575_c)
df$MINK4575_c = ifelse(df$MINK4575_c == 7, 0.82 * 75000, df$MINK4575_c)
df$MINK4575_c = ifelse(df$MINK4575_c == 8, 0.94 * 75000, df$MINK4575_c)
df$MINK4575_c = ifelse(df$MINK4575_c == 9, 1 * 75000, df$MINK4575_c)
# Converting 122k income into value
df$MINK7512_c = ifelse(df$MINK7512 == 1, 0.05 * 122000, df$MINK7512)
df$MINK7512_c = ifelse(df$MINK7512_c == 2, 0.17 * 122000, df$MINK7512_c)
df$MINK7512_c = ifelse(df$MINK7512_c == 3, 0.3 * 122000, df$MINK7512_c)
df$MINK7512_c = ifelse(df$MINK7512_c == 4, 0.43 * 122000, df$MINK7512_c)
df$MINK7512_c = ifelse(df$MINK7512_c == 5, 0.56 * 122000, df$MINK7512_c)
df$MINK7512_c = ifelse(df$MINK7512_c == 6, 0.69 * 122000, df$MINK7512_c)
df$MINK7512_c = ifelse(df$MINK7512_c == 7, 0.82 * 122000, df$MINK7512_c)
df$MINK7512_c = ifelse(df$MINK7512_c == 8, 0.94 * 122000, df$MINK7512_c)
df$MINK7512_c = ifelse(df$MINK7512_c == 9, 1 * 122000, df$MINK7512_c)
# Converting 123k income into value
df$MINK123M_c = ifelse(df$MINK123M == 1, 0.05 * 123000, df$MINK123M)
df$MINK123M_c = ifelse(df$MINK123M_c == 2, 0.17 * 123000, df$MINK123M_c)
df$MINK123M_c = ifelse(df$MINK123M_c == 3, 0.3 * 123000, df$MINK123M_c)
df$MINK123M_c = ifelse(df$MINK123M_c == 4, 0.43 * 123000, df$MINK123M_c)
df$MINK123M_c = ifelse(df$MINK123M_c == 5, 0.56 * 123000, df$MINK123M_c)
df$MINK123M_c = ifelse(df$MINK123M_c == 6, 0.69 * 123000, df$MINK123M_c)
df$MINK123M_c = ifelse(df$MINK123M_c == 7, 0.82 * 123000, df$MINK123M_c)
df$MINK123M_c = ifelse(df$MINK123M_c == 8, 0.94 * 123000, df$MINK123M_c)
df$MINK123M_c = ifelse(df$MINK123M_c == 9, 1 * 123000, df$MINK123M_c)
# Average income
df$MINKGEM_c = (df$MINK123M_c + df$MINK7512_c + df$MINK4575_c + df$MINK3045_c + df$MINKM30_c)/5
#hist(df$MINKGEM_c)
plot_ly(x = ~df$MINKGEM_c, type = "histogram", color = df$MINKGEM_c, colors = c("gold", "blue", "green", "pink", "brown")) %>%
layout(title = "<b>Income Levels<b>")
# Since our data is categorical and gives us levels, after having discussed with the professor, we agreed that a good approach would be convert the income as a categorical variable to numeric one.
# In order to do that, we had to decide a method by which we would do so. The method was decided, mapped out in our excel file
# Here we just brought the calculations done in excel
# The idea was to take the average of income for each category
# From the following histogram we can see that most of customers are skewed towards left but we can see in detail as well that most customers are between 5k - 20K
df$MGEMLEEF_c = ifelse(df$MGEMLEEF == 1, 25, df$MGEMLEEF)
df$MGEMLEEF_c = ifelse(df$MGEMLEEF_c == 2, 35, df$MGEMLEEF_c)
df$MGEMLEEF_c = ifelse(df$MGEMLEEF_c == 3, 45, df$MGEMLEEF_c)
df$MGEMLEEF_c = ifelse(df$MGEMLEEF_c == 4, 55, df$MGEMLEEF_c)
df$MGEMLEEF_c = ifelse(df$MGEMLEEF_c == 5, 65, df$MGEMLEEF_c)
df$MGEMLEEF_c = ifelse(df$MGEMLEEF_c == 4, 75, df$MGEMLEEF_c)
# Just as we did for income, we did the same thing for age well
# The idea and logic behind it was the same as it was behind converting income to numeric
# We believe that age should be numeric because it is best explained when numeric
plot_ly(x = ~df$MINKM30_c, y = ~df$MOSTYPE, type = "box", color = df$MOSTYPE)
# After having converted the categorical variable for income to numerical, here we take income less than 3000 against customer sub-categories and draw a box plot. From the below box plot, we can see that as soon as income crosses 10K, we begint to see a few outliers for certain sub-categories. Another important thing to note is that when income jumps above 20K, the distance of outliers starts to increase. We could say that we are seeing extreme outliers in income levels ranging from 25K to 30K.
# Now the question is if we should keep outliers in our analysis, whether mild or extreme or delete both of them and then proceed?
plot_ly(y = ~df$MOSTYPE, x = ~df$MINK3045_c, type = "box", color = df$MOSTYPE)
plot_ly(x = df$MINK4575_c, y = df$MOSTYPE, type = "box", color = df$MOSTYPE)
prop.table(table(df$CARAVAN))
# When drew a frequency table, we saw that we have a class imbalance problem as 94% of the records in the CARAVAN column are 0s while only 5% of them are 1s.
# This is a problem which needs to be solved first before we begin further
barplot(prop.table(table(df$CARAVAN)), col = rainbow(2), ylim = c(0,1), main = "Class Distribution")
# Here we have shown how the distribution is happening through a bar chart
#testing data
df_test = (df[df$ORIGIN == "test",])
df_test = (df_test[,-1])
nrow(df_test)
table(df_test$CARAVAN)
# training data
df_train = (df[df$ORIGIN == "train",])
df_train = (df_train[,-1])
table(df_train$CARAVAN)
over_train = ovun.sample(CARAVAN ~ ., data =df_train, method = "over", N =10948)$data
table(over_train$CARAVAN)
over_test = ovun.sample(CARAVAN ~ ., data =df_test, method = "over", N =nrow(df_test))$data
table(over_test$CARAVAN)
# we are not fixing sampling problem in test data, we did this because we were having error with doing prediction
for(i in 1:ncol(over_train)){
over_train[,i] <- as.factor(over_train[,i])
}
for(i in 1:ncol(over_train)){
over_train[,i] <- as.factor(over_train[,i])
}
for(i in 1:ncol(over_test)){
over_test[,i] <- as.factor(over_test[,i])
}
over_test$MINKGEM_c = as.numeric(over_test$MINKGEM_c)
over_train$MINKGEM_c = as.numeric(over_train$MINKGEM_c)
over_test$MGEMLEEF_c = as.numeric(over_test$MGEMLEEF_c)
over_train$MGEMLEEF_c = as.numeric(over_train$MGEMLEEF_c)
drewSummary = function(model) {
summary(model)
}
drewMatrix = function(model, test_data) {
predicted = predict(model, test_data, type = "response")
predictedClass = ifelse(predicted>=0.5, 1, 0)
confusionMatrix(as.factor(predictedClass), as.factor(test_data$CARAVAN), positive = "1")
}
drewAnova = function(model1, model2){
anova(model1, model2, test = 'Chisq')
}
drewROC = function(model){
predicted = predict(model, over_test, type = "response")
predictedClass = ifelse(predicted>=0.5, 1, 0)
r = roc(over_test$CARAVAN, predictedClass)
plot.roc(r)
}
set.seed(123)
logit.reg = glm(CARAVAN ~ MOSHOOFD + MGEMOMV + OneHouse + MINKGEM_c+MGEMLEEF_c, data = over_train, family = binomial (link = "logit"))
logit.reg$xlevels[["MGEMOMV"]] <- union(logit.reg$xlevels[["MGEMOMV"]], levels(over_test$MGEMOMV))
drewSummary(logit.reg)
drewMatrix(logit.reg, over_test)
drewROC(logit.reg)
# getRMSE(logit.reg)
# drewAnova(logit.reg)
# we did regression with/out house we see a minimal affect i.e with house model is predicting 60 true positives without it's predicting 65 and it's not significant, so we decided not to include this variable
# difference in deviance =  Null deviance (15177) - 14398 = 872
# From the below confusion matrix for model 1 we see that the accuracy of the model is 11% but the sensitivity is 98% while specificity is 57%. There could be couple of cases to either consider or discard the model. If specificity is the goal then this model is a good fit otherwise, it is not.
train_2 = over_train
train_2 = subset(over_train, select = -c(maintype,subtype,age,noofhouses,noofhousehold,hasThreeHouseHold,OneHouse,moreThanTwoHouse,averageFamily,loners,conservativeFamilies,crusingSeniors,drivenGrowers,grownups,framers,livingWell,retired,successful,MINKM30_c,MINK3045_c,MINK4575_c,MINK7512_c,MINK123M_c,MINKGEM_c,MGEMLEEF_c,MOSHOOFD,MGEMOMV,MAANTHUI,MINKGEM,MGEMLEEF) )
length(train_2)
new_df  = train_2
for(i in 1:ncol(new_df)){
new_df[,i] <- as.integer(new_df[,i])
}
zv = apply(new_df, 2, function(x) length(unique(x)) == 1)
dfr = new_df[, !zv]
n=length(colnames(dfr))
correlationMatrix = cor(dfr[,1:n],use="complete.obs")
summary(correlationMatrix[upper.tri(correlationMatrix)])
# After removing our suspected predictors we still have strong positive correlation with 1% and strong negative corelation with 99.9%, we need to find which of them are highly correlated
high = findCorrelation(correlationMatrix, cutoff = 0.75, names = TRUE)
high
# there are 26 variables which are correlated with each other, before dropping them we need to see how they are correlated with response variable.
length(high)
target_cor_df = data.frame(CARAVAN = cor(df_train[,sort(high)], df_train[, "CARAVAN"]))
cor_df = target_cor_df[order(target_cor_df$CARAVAN,decreasing = T),,drop=F]
excludedVariables = row.names(cor_df[cor_df$CARAVAN < 0.1, ,drop=F])
excludedVariables
paste0("excluding total variables from main data set ", length(excludedVariables))
# There are 24 variables which are less correlated with response variable and having correlation coefficient less than 0.1 so we will exclude them.
train_2 = data.frame(train_2[, !colnames(train_2) %in% excludedVariables])
names(train_2)
length(train_2)
# corrplot(cor(train_3), method = "number")
# 24 + 5 (predictors in model) = 29
# around 29 variables have been excluded from set so far next step would be to find good predictors which are not highly correlated each other and are significant.
# we have reduce dimension from 86 to 57
# corelation between no of car policy and carvan
cor(df_train$APERSAUT, df_train$CARAVAN)
# # corelation between contribution car policies and carvan
cor(df_train$PPERSAUT, df_train$CARAVAN)
# # corelation between  Purchasing power class and carvan
cor(df_train$MKOOPKLA, df_train$CARAVAN)
new_train_2 = train_2
for(i in 1:ncol(new_train_2)){
new_train_2[,i] <- as.numeric(new_train_2[,i])
}
cor_response = data.frame("ind_var" = colnames(new_train_2), "dep_var" = "CARAVAN", "cor_coeff" = 0, "p_values" = 0)
for (i in colnames(new_train_2)){
cor_test <- cor.test(new_train_2[,i], new_train_2[,"CARAVAN"])
cor_response[cor_response$ind_var == i, "correlation_coefficient"] = cor_test$estimate
cor_response[cor_response$ind_var == i, "p_values"] = cor_test$p.value
}
cor_response[order(cor_response$cor_coeff, decreasing = T),]
cor_response = data.frame("ind_var" = colnames(new_train_2), "dep_var" = "CARAVAN", "cor_coeff" = 0, "p_values" = 0)
for (i in colnames(new_train_2)){
cor_test <- cor.test(new_train_2[,i], new_train_2[,"CARAVAN"])
cor_response[cor_response$ind_var == i, "correlation_coefficient"] = cor_test$estimate
cor_response[cor_response$ind_var == i, "p_values"] = cor_test$p.value
}
cor_response[order(cor_response$cor_coeff, decreasing = T),]
options(scipen = 999)
# Here we are seeing the remaining variable's and seeing there significance level with our response variables
cor_response = data.frame("ind_var" = colnames(new_train_2), "dep_var" = "CARAVAN", "cor_coeff" = 0, "p_values" = 0)
for (i in colnames(new_train_2)){
cor_test <- cor.test(new_train_2[,i], new_train_2[,"CARAVAN"])
cor_response[cor_response$ind_var == i, "correlation_coefficient"] = cor_test$estimate
cor_response[cor_response$ind_var == i, "p_values"] = cor_test$p.value
}
cor_response[order(cor_response$cor_coeff, decreasing = T),]
options(scipen = 99)
# Here we are seeing the remaining variable's and seeing there significance level with our response variables
cor_response = data.frame("ind_var" = colnames(new_train_2), "dep_var" = "CARAVAN", "cor_coeff" = 0, "p_values" = 0)
for (i in colnames(new_train_2)){
cor_test <- cor.test(new_train_2[,i], new_train_2[,"CARAVAN"])
cor_response[cor_response$ind_var == i, "correlation_coefficient"] = cor_test$estimate
cor_response[cor_response$ind_var == i, "p_values"] = cor_test$p.value
}
cor_response[order(cor_response$cor_coeff, decreasing = T),]
options(scipen = 9999)
# Here we are seeing the remaining variable's and seeing there significance level with our response variables
corrplot(cor(subset(new_train_2 , select = c(-CARAVAN))), method = "square", type = "upper")
# corelation between Contribution car policies and number of car policies
cor(df_train$PPERSAUT, df_train$APERSAUT)
# There is a high correlation between car policies and number of car policies we will exclude a variable which has less correlation with response variable.
# it's not always necessary to see how much it relates which response variable but it's good as it's tells us how much response variable changes for given predictor.
cor(df_train[ , c("PPERSAUT", "APERSAUT")], df_train[ , "CARAVAN"])
# Contribution car policies is more correlated with response variable so we exclude Number of car policies
train_2 = data.frame(train_2[ , !colnames(train_2) %in% c("APERSAUT")])
paste0("after removing APERSAUT dimension is", length(train_2))
# final variable selected
names(train_2)
length(train_2)
step.wise1 = glm(CARAVAN ~ ., data = train_2, family = binomial(link = "logit"))
#summary(step.wise1)
# difference between null deviance. = 15177 - 12386 = 2791
# step(step.wise1, direction = "backwards")
step.wise1 = glm(CARAVAN ~ ., data = train_2, family = binomial(link = "logit"))
summary(step.wise1)
# difference between null deviance. = 15177 - 12386 = 2791
# step(step.wise1, direction = "backwards")
for(i in 1:ncol(train_2)){
train_2[,i] <- as.factor(train_2[,i])
}
for(i in 1:ncol(train_2)){
train_2[,i] <- as.factor(train_2[,i])
}
# model.2 = glm(formula = CARAVAN ~ MAANTHUI + MGEMOMV + MGEMLEEF + MOSHOOFD +
#     MGODRK + MGODPR + MGODOV + MGODGE + MRELGE + MRELSA + MFWEKIND + MOPLMIDD, family = binomial(link = "logit"), data = train_2)
model.2 = glm(formula = CARAVAN ~ MOSTYPE + MGODRK + MGODPR + MGODOV +
MRELGE + MRELSA + MOPLMIDD + MOPLLAAG + MBERHOOG + MBERZELF +
MBERBOER + MBERMIDD + MBERARBG + MBERARBO + MSKC + MSKD +
MHKOOP + MAUT1 + MAUT2 + MAUT0 + MINK3045 + MINK7512 + MINK123M +
MKOOPKLA + PPERSAUT + PMOTSCO + PVRAAUT + PAANHANG + PWERKT +
PWAOREG + PPLEZIER + AWAPART + AWALAND + ABROM + ALEVEN +
APERSONG + AGEZONG + ABRAND + APLEZIER + AFIETS + ABYSTAND,
family = binomial(link = "logit"), data = over_train)
model.2$xlevels[["MSKD"]] <- union(model.2$xlevels[["MSKD"]], levels(over_test$MSKD))
model.2$xlevels[["MAUT2"]] <- union(model.2$xlevels[["MAUT2"]], levels(over_test$MAUT2))
model.2$xlevels[["MINK123M"]] <- union(model.2$xlevels[["MINK123M"]], levels(over_test$MINK123M))
model.2$xlevels[["PPERSAUT"]] <- union(model.2$xlevels[["PPERSAUT"]], levels(over_test$PPERSAUT))
model.2$xlevels[["PVRAAUT"]] <- union(model.2$xlevels[["PVRAAUT"]], levels(over_test$PVRAAUT))
model.2$xlevels[["PWERKT"]] <- union(model.2$xlevels[["PWERKT"]], levels(over_test$PWERKT))
model.2$xlevels[["ABROM"]] <- union(model.2$xlevels[["ABROM"]], levels(over_test$ABROM))
model.2$xlevels[["ALEVEN"]] <- union(model.2$xlevels[["ALEVEN"]], levels(over_test$ALEVEN))
model.2$xlevels[["ABRAND"]] <- union(model.2$xlevels[["ABRAND"]], levels(over_test$ABRAND))
model.2$xlevels[["AFIETS"]] <- union(model.2$xlevels[["AFIETS"]], levels(over_test$AFIETS))
drewSummary(model.2)
drewMatrix(model.2, over_test)
# difference in deviance =  Null deviance (15177.2) - 9766.3 = 5411
# Sensitivity : 64%
# Accuracy : 54%
corrplot(cor(subset(df_train , select = c("PBRAND", "MOSTYPE", "PPERSAUT", "MKOOPKLA", "MHKOOP", "CARAVAN"))), method = "number", type = "upper")
train_3 = over_train
for(i in 1:ncol(train_3)){
train_3[,i] <- as.factor(train_3[,i])
}
model.3 = glm(formula = CARAVAN ~ PBRAND + MOSTYPE + PPERSAUT + MKOOPKLA+MHKOOP, family = binomial(link = "logit"),
data = train_3)
model.3$xlevels[["PPERSAUT"]] <- union(model.3$xlevels[["PPERSAUT"]], levels(over_test$PPERSAUT))
predicted_3 = predict(model.3, over_test, type = "response")
predictedClass_3 = ifelse(predicted_3>=0.5, 1, 0)
drewSummary(model.3)
drewMatrix(model.3, over_test)
getRMSE(predictedClass_3)
# Area under curve
pr <- prediction(predictedClass_3,over_test$CARAVAN)
perf <- performance(pr,measure = "tpr",x.measure = "fpr")
plot(perf) > auc(over_test$CARAVAN,predictedClass_3)
auc_ROCR <- performance(pr, measure = "auc")
auc_ROCR <- auc_ROCR@y.values[[1]]
auc_ROCR
pR2(model.3)['McFadden']
# difference in deviance =  Null deviance (15177) - 12051 = 3025
# sensivity 72%
# Accuracy : 60%
auc_ROCR
pred_t <- predict(model.3, na.action=na.pass)
hist(pred_t)
boxplot(pred_t)
##Plotting residual histograms for training and validation data
resid.t<-residuals(model.3)
hist(resid.t)
pred_t <- predict(model.3, na.action=na.pass)
hist(pred_t)
boxplot(pred_t)
##Plotting residual histograms for training and validation data
# resid.t<-residuals(model.3)
# hist(resid.t)
pred_t <- predict(model.3, na.action=na.pass)
hist(pred_t)
boxplot(pred_t)
##Plotting residual histograms for training and validation data
resid.t<-residuals(model.3)
hist(resid.t)
drewROC(model.3)
lift.example <- lift(relevel(as.factor(over_test$CARAVAN), ref="1") ~ predicted_3, data = over_test)
#xyplot(lift.example, plot = "gain")
library(gains)
actual = as.numeric(over_test$CARAVAN)
predicted_3_num = as.numeric(predicted_3)
gain = gains(actual, predicted_3_num)
barplot(gain$mean.resp / mean(actual), names.arg = gain$depth, xlab = "Percentile", ylab = "Mean Response", main = "Decile-wise lift chart")
train_4 = over_train
for(i in 1:ncol(train_4)){
train_4[,i] <- as.factor(train_4[,i])
}
fit1 = rpart(formula=CARAVAN ~ .,data=over_train,method = 'class', control=rpart.control(minsplit=20, minbucket=1, cp=0.008))
fit1
printcp(fit1)
fancyRpartPlot(fit1)
glm_6 = glm(formula = CARAVAN ~  MINKGEM+PBRAND+PPERSAUT, family = binomial(link = "logit"),data = train_4)
glm_6$xlevels[["PPERSAUT"]] <- union(glm_6$xlevels[["PPERSAUT"]], levels(over_test$PPERSAUT))
summary(glm_6)
predicted_6 = predict(glm_6, over_test, type = "response")
predictedClass_6 = ifelse(predicted_6>=0.5, 1, 0)
confusionMatrix(as.factor(predictedClass_6), as.factor(over_test$CARAVAN), positive = "1")
accuracy(predictedClass_6, as.numeric(over_test$CARAVAN))
pR2(glm_6)['McFadden']
# difference in deviance =  Null deviance (15177) - 12069 = 3108
# Accuracy 68%
# Sensitivity 69%
fit1 = rpart(formula=CARAVAN ~ .,data=over_train,method = 'class', control=rpart.control(minsplit=20, minbucket=1, cp=0.008))
fit1
printcp(fit1)
fancyRpartPlot(fit1)
glm_6 = glm(formula = CARAVAN ~  MINKGEM+PBRAND+PPERSAUT, family = binomial(link = "logit"),data = train_4)
glm_6$xlevels[["PPERSAUT"]] <- union(glm_6$xlevels[["PPERSAUT"]], levels(over_test$PPERSAUT))
summary(glm_6)
predicted_6 = predict(glm_6, over_test, type = "response")
predictedClass_6 = ifelse(predicted_6>=0.4, 1, 0)
confusionMatrix(as.factor(predictedClass_6), as.factor(over_test$CARAVAN), positive = "1")
accuracy(predictedClass_6, as.numeric(over_test$CARAVAN))
pR2(glm_6)['McFadden']
# difference in deviance =  Null deviance (15177) - 12069 = 3108
# Accuracy 68%
# Sensitivity 58%
fit1 = rpart(formula=CARAVAN ~ .,data=over_train,method = 'class', control=rpart.control(minsplit=20, minbucket=1, cp=0.008))
fit1
printcp(fit1)
fancyRpartPlot(fit1)
glm_6 = glm(formula = CARAVAN ~  MINKGEM+PBRAND+PPERSAUT, family = binomial(link = "logit"),data = train_4)
glm_6$xlevels[["PPERSAUT"]] <- union(glm_6$xlevels[["PPERSAUT"]], levels(over_test$PPERSAUT))
summary(glm_6)
predicted_6 = predict(glm_6, over_test, type = "response")
predictedClass_6 = ifelse(predicted_6>=0.5, 1, 0)
confusionMatrix(as.factor(predictedClass_6), as.factor(over_test$CARAVAN), positive = "1")
accuracy(predictedClass_6, as.numeric(over_test$CARAVAN))
pR2(glm_6)['McFadden']
# difference in deviance =  Null deviance (15177) - 12069 = 3108
# Accuracy 68%
# Sensitivity 58%
drewROC(glm_6)
anova(logit.reg,model.2,test = 'Chisq')
anova(model.3,glm_6,test = 'Chisq')
