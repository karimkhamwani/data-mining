---
title: "Final Project"
author: "Karim and Mustafa"
date: "10/27/2022"
output: html_document
---

```{r setup, include=TRUE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
# install.packages("Hmisc")
# install.packages("pastecs")
# install.packages("ggplot2")
# install.packages("Hmisc")
# install.packages("fastDummies")
# install.packages("lmtest")
# install.packages("lmtest")
# install.packages("caretEnsemble")
# install.packages("Amelia")
# install.packages("GGally")
```


```{r}
library(ggplot2)
library(psych)
library(corrplot)
library(caret)
library(glmnet)
library(leaps)
library(reshape2)
library(gridExtra)
library(fastDummies)
library(lmtest)
library(pastecs)
library(skimr)
library(tidyverse)
library(caret)
library(caretEnsemble)
library(psych)
library(Amelia)
library(mice)
library(GGally)
library(rpart)
library(randomForest)
library(nnet)
library(ROCR)
library(Metrics)
library(caret)
library(pscl)
library(forecast)
library(rpart)
library(rattle)
library(ggplot2)
library(plyr)
library(rlist)
library(pROC)
library(ROSE)
library(plotly)
```




```{r}
# reading file
df = readr::read_csv("caravan-insurance-challenge.csv")
```


# Step 1 Data Visulation and exploration

```{r}
dimension = dim(df)
# total number of observations are 9822
# total number of variables are 87 

totalRows = dimension[1]

# data type of 86 variables is number and 1 variable is char, our target variable is number of policies bought remaining variables are categorical numeric.

head(df)
# explore top 5 rows
```

# Descriptive analysis
```{r}
# summary of main data set
# stat.desc(df)
# summary(df)

# missing values in main data set
paste0("Total missing values:", sum(is.na(df)))
```

```{r}
desc = skim(df)
```


# Out liers
```{r}
plot_ly(x = ~df$MINKM30_c, y = ~df$MOSTYPE, type = "box", color = df$MOSTYPE)

# After having converted the categorical variable for income to numerical, here we take income less than 3000 against customer sub-categories and draw a box plot. From the below box plot, we can see that as soon as income crosses 10K, we begint to see a few outliers for certain sub-categories. Another important thing to note is that when income jumps above 20K, the distance of outliers starts to increase. We could say that we are seeing extreme outliers in income levels ranging from 25K to 30K. 

# Now the question is if we should keep outliers in our analysis, whether mild or extreme or delete both of them and then proceed?
```

```{r}
plot_ly(y = ~df$MOSTYPE, x = ~df$MINK3045_c, type = "box", color = df$MOSTYPE)
```

```{r}
plot_ly(x = df$MINK4575_c, y = df$MOSTYPE, type = "box", color = df$MOSTYPE)
```

# Plot graphs
```{r}
unInsuredRows = df[df$CARAVAN == 0,]
insuredRows = df[df$CARAVAN == 1,]

peopleNotInsuredWithCaravan  = nrow(unInsuredRows);
peopleInsuredWithCaravan = nrow(insuredRows);


ratioOfPeopleWhoDontBought = peopleNotInsuredWithCaravan/totalRows * 100
ratioOfPeopleWhoBought = peopleInsuredWithCaravan/totalRows * 100
# 5.96% who bought insurance vs 94% who didn't buy the insurance, by statistics it looks like company is in loss people are not interested in buying they have to something to improve.

dat <- data.frame(
  policy_status = factor(c("Not Insured","Insured"), levels=c("Not Insured","Insured")),
  Count = c( peopleNotInsuredWithCaravan , peopleInsuredWithCaravan)
)

ggplot(data=dat, aes(x=policy_status, y=Count, fill=policy_status)) +
  geom_bar(colour="red", stat="identity")

```

# Customer Main Type
```{r}
ggplot(df,aes(factor(df$MOSHOOFD))) + 
    geom_bar(aes(fill = factor(df$CARAVAN))) + 
    labs(x="Customer Main type") +
    scale_fill_discrete(name = "CARAVAN") + 
    ggtitle("Caravan Policy based on Customer Main Type") +
    theme(plot.title = element_text(hjust = 0.5))
df$maintype = df$MOSHOOFD

nrow(df[df$maintype == 1 & df$CARAVAN == 1,])
nrow(df[df$maintype == 2 & df$CARAVAN == 1,])
nrow(df[df$maintype == 3 & df$CARAVAN == 1,])
nrow(df[df$maintype == 4 & df$CARAVAN == 1,])
nrow(df[df$maintype == 5 & df$CARAVAN == 1,])
nrow(df[df$maintype == 6 & df$CARAVAN == 1,])
nrow(df[df$maintype == 7 & df$CARAVAN == 1,])
nrow(df[df$maintype == 8 & df$CARAVAN == 1,])
nrow(df[df$maintype == 9 & df$CARAVAN == 1,])
nrow(df[df$maintype == 10 & df$CARAVAN == 1,])

# Here we wanted to see the which customer main type has the highest frequency/count of buying the insurance. Based on results, we see that there are atleast 4 main customer categories that buy insurance. However, for our ease and understanding purposes we will only consider the top 2. This brings us to select, category number 8 and 3, where 8 = Family with grown ups and 3 = Driven Growths
```


# Customer sub type
```{r}
ggplot(df,aes(factor(df$MOSTYPE))) + 
    geom_bar(aes(fill = factor(df$CARAVAN))) + 
    labs(x="Customer Sub type") +
    scale_fill_discrete(name = "CARAVAN") + 
    ggtitle("Policy Bought based on Customer sub Type") +
    theme(plot.title = element_text(hjust = 0.5))

df$subtype = df$MOSTYPE
nrow(df[df$subtype == 33 & df$CARAVAN == 1,])
nrow(df[df$subtype == 8 & df$CARAVAN == 1,])

# category 33 and 8 purchased more policies.
# Based on our main category which compromises of various sub-categories, we can see that sub-categories number 33 and 8 are porne to buying insurance. These should be considered as the characteristics/attributes of the types of customers that exist in the main customer category. hence, we could say that, those who are middle class and those who are low class but have large families have a higher chance of getting the insurance 
```


# Age
```{r}
ggplot(df,aes(factor(df$MGEMLEEF))) + 
    geom_bar(aes(fill = factor(df$CARAVAN))) + 
    geom_text(stat='count', aes(label=..count..), vjust=0) + 
    labs(x="Age Group") +
    scale_fill_discrete(name = "CARAVAN") + 
    ggtitle("Policy bought on age group") +
    theme(plot.title = element_text(hjust = 0.5))

df$age = df$MGEMLEEF
nrow(df[df$age == 1 & df$CARAVAN == 1,])
nrow(df[df$age == 2 & df$CARAVAN == 1,])
nrow(df[df$age == 3 & df$CARAVAN == 1,])
nrow(df[df$age == 4 & df$CARAVAN == 1,])
nrow(df[df$age == 5 & df$CARAVAN == 1,])
nrow(df[df$age == 6 & df$CARAVAN == 1,])

# Here we have explored to see what is the age range of the customers that buy the insurance. Based on our analysis we see that customers who are between the ages 40-50 are prone to buying insurance compared with others. Hence, we could say that it is among the many characteristics of the main customer group [3,8]

```


# No of houses
```{r}
ggplot(df,aes(factor(df$MAANTHUI))) + 
    geom_bar(aes(fill = factor(df$CARAVAN))) + 
    geom_text(stat='count', aes(label=..count..), vjust=0) + 
    labs(x="Number of houses") +
    scale_fill_discrete(name = "CARAVAN") + 
    ggtitle("Number of houses customer has who bought insurance") +
    theme(plot.title = element_text(hjust = 0.5))

df$noofhouses = df$MAANTHUI
nrow(df[df$noofhouses == 1 & df$CARAVAN == 1,])
nrow(df[df$noofhouses == 2 & df$CARAVAN == 1,])
nrow(df[df$noofhouses == 3 & df$CARAVAN == 1,])

# Now we wanted to see who is prone to getting an insurance with respect to number of houses and we have found that customers having at least 1 house are likely to get the insurance.
```


# No of house hold
```{r}
ggplot(df,aes(factor(df$MGEMOMV))) + 
    geom_bar(aes(fill = factor(df$CARAVAN))) + 
    geom_text(stat='count', aes(label=..count..), vjust=0) + 
    labs(x="Number of house hold") +
    scale_fill_discrete(name = "CARAVAN") + 
    ggtitle("Number of house hold") +
    theme(plot.title = element_text(hjust = 0.5))

df$noofhousehold = df$MGEMOMV
nrow(df[df$noofhousehold == 1 & df$CARAVAN == 1,])
nrow(df[df$noofhousehold == 2 & df$CARAVAN == 1,])
nrow(df[df$noofhousehold == 3 & df$CARAVAN == 1,])
df$hasThreeHouseHold = ifelse(df$noofhousehold == 3, 1, 0)
# Now we wanted to see who is prone to getting an insurance with respect to number of houses and we have found that customers having at least 1 house are likely to get the insurance.
```

# Charactertics we found so far
# Customer having 3 house hold
# Customer have one house
# Age of customer is between 40 to 50
# Customer are Driven Growers
# Customer belongs to Lower class large families


# Correlation Analysis
```{r}
corrplot(cor(df[, c("subtype","maintype", "age", "noofhouses", "noofhousehold", "CARAVAN")]), method = "number")


# From the correlation matrix we have below, we have some interesting insights. The reason to run the correlation matrix was to figure out variables of interest which might cause either overfitting or underfitting. # Here we can see that the variables of interest are positively correlated. One exception to this is the correlation between age and number of households. We see that there is a negative correlation between it. Which actually makes sense because, the greater the age, the no of households will decrease. 
```


# Which varaible to select when they are highly postive correlated?

```{r}
cor(df$subtype, df$CARAVAN)
cor(df$maintype, df$CARAVAN)
# we will choose which have high corelation with response variable in this case both are weak correlated doesn't matter what we really choose
# we will choose maintype
```


# Categorizing Data

# Number of Houses
```{r}
table(df$MAANTHUI)

df$MAANTHUI = replace(df$MAANTHUI, df$MAANTHUI > 2, 2)
df$OneHouse = ifelse(df$MAANTHUI ==1, 1, 0)
df$moreThanTwoHouse = ifelse(df$MAANTHUI > 2, 1, 0)


# by looking frequencies of houses we categorized houses into dummies.
```

# Grouping sub customer to meanful customer type.
```{r}
df$averageFamily = ifelse(df$MOSTYPE %in% c(12,11,9,10,13), 1, 0)
df$loners = ifelse(df$MOSTYPE %in% c(17,15,18,16,19), 1, 0)
df$conservativeFamilies = ifelse(df$MOSTYPE %in% c(39,38), 1, 0)
df$crusingSeniors = ifelse(df$MOSTYPE %in% c(26,25,28,27), 1, 0)
df$drivenGrowers = ifelse(df$MOSTYPE %in% c(6,7,8), 1, 0)
df$grownups = ifelse(df$MOSTYPE %in% c(33,34,35,36,37), 1, 0)
df$framers = ifelse(df$MOSTYPE %in% c(40,41), 1, 0)
df$livingWell = ifelse(df$MOSTYPE %in% c(20,21,22,23,24), 1, 0)
df$retired = ifelse(df$MOSTYPE %in% c(29,30,31,32), 1, 0)
df$successful = ifelse(df$MOSTYPE %in% c(1,2,3,4,5), 1, 0)

dat <- data.frame(
  Categorized_Customers = factor(c("averageFamily", "loners", "conservativeFamilies", "crusingSeniors", "drivenGrowers", "grownups", "framers", "livingWell", "retired", "successful"), levels=c("averageFamily", "loners", "conservativeFamilies", "crusingSeniors", "drivenGrowers", "grownups", "framers", "livingWell", "retired", "successful")),
  Count = c( sum(df$averageFamily), sum(df$loners), sum(df$conservativeFamilies), sum(df$crusingSeniors), sum(df$drivenGrowers), sum(df$grownups), sum(df$framers), sum(df$livingWell), sum(df$retired), sum(df$successful) )
)

ggplot(data=dat, aes(x=Categorized_Customers, y=Count, fill=Categorized_Customers)) +
  geom_bar(colour="red", stat="identity")

```


# Income Conversion
```{r}
# Converting 30k income into value
df$MINKM30_c = ifelse(df$MINKM30 == 1, 0.05 * 30000, df$MINKM30)
df$MINKM30_c = ifelse(df$MINKM30_c == 2, 0.17 * 30000, df$MINKM30_c)
df$MINKM30_c = ifelse(df$MINKM30_c == 3, 0.3 * 30000, df$MINKM30_c)
df$MINKM30_c = ifelse(df$MINKM30_c == 4, 0.43 * 30000, df$MINKM30_c)
df$MINKM30_c = ifelse(df$MINKM30_c == 5, 0.56 * 30000, df$MINKM30_c)
df$MINKM30_c = ifelse(df$MINKM30_c == 6, 0.69 * 30000, df$MINKM30_c)
df$MINKM30_c = ifelse(df$MINKM30_c == 7, 0.82 * 30000, df$MINKM30_c)
df$MINKM30_c = ifelse(df$MINKM30_c == 8, 0.94 * 30000, df$MINKM30_c)
df$MINKM30_c = ifelse(df$MINKM30_c == 9, 1 * 30000, df$MINKM30_c)


# Converting 45k income into value
df$MINK3045_c = ifelse(df$MINK3045 == 1, 0.05 * 45000, df$MINK3045)
df$MINK3045_c = ifelse(df$MINK3045_c == 2, 0.17 * 45000, df$MINK3045_c)
df$MINK3045_c = ifelse(df$MINK3045_c == 3, 0.3 * 45000, df$MINK3045_c)
df$MINK3045_c = ifelse(df$MINK3045_c == 4, 0.43 * 45000, df$MINK3045_c)
df$MINK3045_c = ifelse(df$MINK3045_c == 5, 0.56 * 45000, df$MINK3045_c)
df$MINK3045_c = ifelse(df$MINK3045_c == 6, 0.69 * 45000, df$MINK3045_c)
df$MINK3045_c = ifelse(df$MINK3045_c == 7, 0.82 * 45000, df$MINK3045_c)
df$MINK3045_c = ifelse(df$MINK3045_c == 8, 0.94 * 45000, df$MINK3045_c)
df$MINK3045_c = ifelse(df$MINK3045_c == 9, 1 * 45000, df$MINK3045_c)

# Converting 70k income into value
df$MINK4575_c = ifelse(df$MINK4575 == 1, 0.05 * 75000, df$MINK4575)
df$MINK4575_c = ifelse(df$MINK4575_c == 2, 0.17 * 75000, df$MINK4575_c)
df$MINK4575_c = ifelse(df$MINK4575_c == 3, 0.3 * 75000, df$MINK4575_c)
df$MINK4575_c = ifelse(df$MINK4575_c == 4, 0.43 * 75000, df$MINK4575_c)
df$MINK4575_c = ifelse(df$MINK4575_c == 5, 0.56 * 75000, df$MINK4575_c)
df$MINK4575_c = ifelse(df$MINK4575_c == 6, 0.69 * 75000, df$MINK4575_c)
df$MINK4575_c = ifelse(df$MINK4575_c == 7, 0.82 * 75000, df$MINK4575_c)
df$MINK4575_c = ifelse(df$MINK4575_c == 8, 0.94 * 75000, df$MINK4575_c)
df$MINK4575_c = ifelse(df$MINK4575_c == 9, 1 * 75000, df$MINK4575_c)

# Converting 122k income into value
df$MINK7512_c = ifelse(df$MINK7512 == 1, 0.05 * 122000, df$MINK7512)
df$MINK7512_c = ifelse(df$MINK7512_c == 2, 0.17 * 122000, df$MINK7512_c)
df$MINK7512_c = ifelse(df$MINK7512_c == 3, 0.3 * 122000, df$MINK7512_c)
df$MINK7512_c = ifelse(df$MINK7512_c == 4, 0.43 * 122000, df$MINK7512_c)
df$MINK7512_c = ifelse(df$MINK7512_c == 5, 0.56 * 122000, df$MINK7512_c)
df$MINK7512_c = ifelse(df$MINK7512_c == 6, 0.69 * 122000, df$MINK7512_c)
df$MINK7512_c = ifelse(df$MINK7512_c == 7, 0.82 * 122000, df$MINK7512_c)
df$MINK7512_c = ifelse(df$MINK7512_c == 8, 0.94 * 122000, df$MINK7512_c)
df$MINK7512_c = ifelse(df$MINK7512_c == 9, 1 * 122000, df$MINK7512_c)

# Converting 123k income into value
df$MINK123M_c = ifelse(df$MINK123M == 1, 0.05 * 123000, df$MINK123M)
df$MINK123M_c = ifelse(df$MINK123M_c == 2, 0.17 * 123000, df$MINK123M_c)
df$MINK123M_c = ifelse(df$MINK123M_c == 3, 0.3 * 123000, df$MINK123M_c)
df$MINK123M_c = ifelse(df$MINK123M_c == 4, 0.43 * 123000, df$MINK123M_c)
df$MINK123M_c = ifelse(df$MINK123M_c == 5, 0.56 * 123000, df$MINK123M_c)
df$MINK123M_c = ifelse(df$MINK123M_c == 6, 0.69 * 123000, df$MINK123M_c)
df$MINK123M_c = ifelse(df$MINK123M_c == 7, 0.82 * 123000, df$MINK123M_c)
df$MINK123M_c = ifelse(df$MINK123M_c == 8, 0.94 * 123000, df$MINK123M_c)
df$MINK123M_c = ifelse(df$MINK123M_c == 9, 1 * 123000, df$MINK123M_c)

# Average income
df$MINKGEM_c = (df$MINK123M_c + df$MINK7512_c + df$MINK4575_c + df$MINK3045_c + df$MINKM30_c)/5
hist(df$MINKGEM_c)

```

# Converting age into numerical.
```{r}
df$MGEMLEEF_c = ifelse(df$MGEMLEEF == 1, 25, df$MGEMLEEF)
df$MGEMLEEF_c = ifelse(df$MGEMLEEF_c == 2, 35, df$MGEMLEEF_c)
df$MGEMLEEF_c = ifelse(df$MGEMLEEF_c == 3, 45, df$MGEMLEEF_c)
df$MGEMLEEF_c = ifelse(df$MGEMLEEF_c == 4, 55, df$MGEMLEEF_c)
df$MGEMLEEF_c = ifelse(df$MGEMLEEF_c == 5, 65, df$MGEMLEEF_c)
df$MGEMLEEF_c = ifelse(df$MGEMLEEF_c == 4, 75, df$MGEMLEEF_c)
```

```{r}
# MOSTYPE : customer subtype
# MFWEKIND: Household with children
# MOPLLAAG: Lower level education
# MHHUUR :  Rented house
# MHKOOP :  Home owners
# MINKM30:  Income < 30.000 low income
# MINK7512: Income 75-122.000 high income
# MKOOPKLA: Purchasing power class
# PPERSAUT: Contribution car policies
# CARAVAN:  Number of mobile home policies 0 - 1
```


# Class Im balance problem
```{r}
prop.table(table(df$CARAVAN))
# 94% people didn't buy insurance and only 5% bought. under sampling problem

barplot(prop.table(table(df$CARAVAN)), col = rainbow(2), ylim = c(0,0.7), main = "Class Distribution")
```


# Partition data

```{r}
# training data
df_train = (df[df$ORIGIN == "train",])
df_train = (df_train[,-1])
nrow(df_train)
```


```{r}
#testing data
df_test = (df[df$ORIGIN == "test",])
df_test = (df_test[,-1])
nrow(df_test)
table(df_test$CARAVAN)
```



```{r}
over_train = ovun.sample(CARAVAN ~ ., data =df_train, method = "over", N =10948)$data
table(over_train$CARAVAN)
```

```{r}
over_test = ovun.sample(CARAVAN ~ ., data =df_test, method = "over", N =nrow(df_test))$data
table(over_test$CARAVAN)
# we are not fixing sampling problem in test data, i did this becuase i was having error with doing prediction
```


# Convert train data set into factor
```{r}
for(i in 1:ncol(over_train)){
over_train[,i] <- as.factor(over_train[,i])
}
```


# Convert test data set into factor
```{r}
for(i in 1:ncol(over_test)){
over_test[,i] <- as.factor(over_test[,i])
}
```



```{r}
over_test$MINKGEM_c = as.numeric(over_test$MINKGEM_c)
over_train$MINKGEM_c = as.numeric(over_train$MINKGEM_c)

over_test$MGEMLEEF_c = as.numeric(over_test$MGEMLEEF_c)
over_train$MGEMLEEF_c = as.numeric(over_train$MGEMLEEF_c)

```



# Draw Confusion matrix function

```{r}
drewSummary = function(model) {
  summary(model)
}
```


```{r}
drewMatrix = function(model, test_data) {
   predicted = predict(model, test_data, type = "response")
   predictedClass = ifelse(predicted>=0.5, 1, 0)
   confusionMatrix(as.factor(predictedClass), as.factor(test_data$CARAVAN), positive = "1")
}
```

```{r}
drewAnova = function(model1, model2){
    anova(model1, model2, test = 'Chisq')
}
```



# Model 1
```{r}
set.seed(123)

logit.reg = glm(CARAVAN ~ MOSHOOFD + MGEMOMV + OneHouse + MINKGEM_c+MGEMLEEF_c, data = over_train, family = binomial (link = "logit"))

logit.reg$xlevels[["MGEMOMV"]] <- union(logit.reg$xlevels[["MGEMOMV"]], levels(over_test$MGEMOMV))

drewSummary(logit.reg)
drewMatrix(logit.reg, over_test)
# drewAnova(logit.reg)

# we did regression with/out house we see a minimal affect i.e with house model is predicting 60 true positives without it's predicting 65 and it's not significant, so we decided not to include this variable.
# difference in deviance =  Null deviance (15177) - 14398 = 870
```


# Building model using correlation Analysis
# Pre processing
```{r}

train_2 = over_train

new_df  = data.frame(df_train[, -c(2,3,4,5)])
zv = apply(new_df, 2, function(x) length(unique(x)) == 1)
dfr = new_df[, !zv]
n=length(colnames(dfr))
correlationMatrix = cor(dfr[,1:n],use="complete.obs")

summary(correlationMatrix[upper.tri(correlationMatrix)])

# After removing our suspected predictors we still have strong positive correlation with 1% and strong negative corelation with 99.9%, we need to find which of them are highly corelated
```

```{r}
high = findCorrelation(correlationMatrix, cutoff = 0.75, names = TRUE)
high
# there are 34 variables which are correlated with each other, before dropping them we need to see how they are correlated with response variable.
```


```{r}
target_cor_df = data.frame(CARAVAN = cor(df_train[,sort(high)], df_train[, "CARAVAN"]))


cor_df = target_cor_df[order(target_cor_df$CARAVAN,decreasing = T),,drop=F]

excludedVariables = row.names(cor_df[cor_df$CARAVAN < 0.1, ,drop=F])

excludedDummiesAndVariables = list.append(excludedVariables, 'maintype', 'age', 'noofhouses','hasThreeHouseHold', 'moreThanTwoHouse', 'averageFamily', 'loners', 'conservativeFamilies', 'crusingSeniors', 'drivenGrowers', 'grownups', 'framers','livingWell', 'retired', 'successful','MINK7512_c', 'MINK123M_c', 'MGEMLEEF_c')

excludedDummiesAndVariables
# There are 33 variables which are less correlated with response variable and having correlation coefficient less than 0.1 so we will exclude them.
```


```{r}
train_2 = data.frame(train_2[, !colnames(train_2) %in% excludedDummiesAndVariables])
names(train_2)
# corrplot(cor(train_3), method = "number")
# 33 + 4 + 1 (carvan) = 38
# around 37 variables have been excluded from set so far next step would be to find good predictors which are not highly correlated each other and are significant.
```


```{r}
# corelation between no of car policy and carvan
cor(df_train$APERSAUT, df_train$CARAVAN)
# # corelation between contribution car policies and carvan
cor(df_train$PPERSAUT, df_train$CARAVAN)
# # corelation between  Purchasing power class and carvan
cor(df_train$MKOOPKLA, df_train$CARAVAN)
```

```{r}
new_train_2 = train_2
for(i in 1:ncol(new_train_2)){
new_train_2[,i] <- as.numeric(new_train_2[,i])
}
```



```{r}
cor_response = data.frame("ind_var" = colnames(new_train_2), "dep_var" = "CARAVAN", "cor_coeff" = 0, "p_values" = 0)

for (i in colnames(new_train_2)){
    cor_test <- cor.test(new_train_2[,i], new_train_2[,"CARAVAN"])
    cor_response[cor_response$ind_var == i, "correlation_coefficient"] = cor_test$estimate
    cor_response[cor_response$ind_var == i, "p_values"] = cor_test$p.value
}
cor_response[order(cor_response$cor_coeff, decreasing = T),]
```


```{r}
corrplot(cor(subset(df_train , select = c(-CARAVAN))), method = "square", type = "upper")
```


```{r}
# corelation between Contribution car policies and number of car policies 
cor(df_train$PPERSAUT, df_train$APERSAUT)
# There is a high correlation between car policies and number of car policies we will exclude a variable which has less correlation with response variable.
# it's not always necessary to see how much it relates which response variable but it's good as it's tells us how much response variable changes for given predictor.
cor(df_train[ , c("PPERSAUT", "APERSAUT")], df_train[ , "CARAVAN"])
# Contribution car policies is more correlated with response variable so we exclude Number of car policies
train_2 = data.frame(train_2[ , !colnames(train_2) %in% c("APERSAUT")])
```


```{r}
step.wise1 = glm(CARAVAN ~ ., data = train_2, family = binomial(link = "logit"))
summary(step.wise1)
#step(step.wise1, direction = "forward")
```


```{r}
# MAANTHUI + MGEMOMV + MGEMLEEF + MOSHOOFD + 
#     MGODRK + MGODPR + MGODOV + MGODGE + MRELGE + MRELSA + MFALLEEN + 
#     MFGEKIND + MFWEKIND + MOPLHOOG + MOPLMIDD + MOPLLAAG + MBERHOOG + 
#     MBERZELF + MBERBOER + MBERMIDD + MBERARBG + MBERARBO + MSKA + 
#     MSKB1 + MSKB2 + MSKC + MHHUUR + MAUT1 + MAUT2 + MAUT0 + 
#     MZFONDS + MINKM30 + MINK3045 + MINK4575 + MINKGEM + MKOOPKLA + 
#     PWALAND + PPERSAUT + PBESAUT + PBROM + PZEILPL + PFIETS + 
#     PINBOED + AWAPART + AWABEDR + AMOTSCO + AVRAAUT + AAANHANG + 
#     ATRACTOR + AWERKT + ALEVEN + APERSONG + AGEZONG + AWAOREG + 
#     ABRAND + APLEZIER + ABYSTAND

# List of variables suggested by stepwise foward by most to least siginificat we will pick first few and run the model so we avoid over fitting model
```


# Model 2
```{r}
model.2 = glm(formula = CARAVAN ~ MAANTHUI + MGEMOMV + MGEMLEEF + MOSHOOFD + 
    MGODRK + MGODPR + MGODOV + MGODGE + MRELGE + MRELSA + MFWEKIND + MOPLMIDD, family = binomial(link = "logit"), data = train_2)

model.2$xlevels[["MGEMOMV"]] <- union(model.2$xlevels[["MGEMOMV"]], levels(over_test$MGEMOMV))

drewSummary(model.2)
drewMatrix(model.2, over_test)
# difference in deviance =  Null deviance (15177) - 13808 = 1369
```
# explanation

# when removing MBERMIDD (middle management) from predictors Sensitivity jump from 45 to 51 while Accuracy is constant 0.62


# Model 3 with Domain knowledge
```{r}
corrplot(cor(subset(df_train , select = c("PBRAND", "MOSTYPE", "PPERSAUT", "MKOOPKLA", "MHKOOP", "CARAVAN"))), method = "number", type = "upper")
```

```{r}

model.3 = glm(formula = CARAVAN ~ PBRAND + MOSTYPE + PPERSAUT + MKOOPKLA+MHKOOP, family = binomial(link = "logit"), 
    data = over_train)

model.3$xlevels[["PPERSAUT"]] <- union(model.3$xlevels[["PPERSAUT"]], levels(over_test$PPERSAUT))

predicted_3 = predict(model.3, over_test, type = "response")
predictedClass_3 = ifelse(predicted_3>=0.5, 1, 0)

drewSummary(model.3)
drewMatrix(model.3, over_test)

# Area under curve
pr <- prediction(predictedClass_3,over_test$CARAVAN)
perf <- performance(pr,measure = "tpr",x.measure = "fpr")
plot(perf) > auc(over_test$CARAVAN,predictedClass_3)

auc_ROCR <- performance(pr, measure = "auc")
auc_ROCR <- auc_ROCR@y.values[[1]]
auc_ROCR

pR2(model.3)['McFadden']
accuracy(predictedClass_3, as.numeric(over_test$CARAVAN))
# difference in deviance =  Null deviance (15177) - 12152 = 3025
```
Explanation
AUC roc plot logistic regression Our AUC score is 0.7176653


# Evaluting performance of model 3
```{r}
pred_t <- predict(model.3, na.action=na.pass)
hist(pred_t)
boxplot(pred_t)

##Plotting residual histograms for training and validation data
resid.t<-residuals(model.3)
hist(resid.t)
```

# Roc
```{r}
r <- roc(over_test$CARAVAN, predicted_3)
plot.roc(r)
```

# Lift chart
```{r}
lift.example <- lift(relevel(as.factor(over_test$CARAVAN), ref="1") ~ predicted_3, data = over_test)
#xyplot(lift.example, plot = "gain")
```

# Decil Wise chart
```{r}
library(gains)
actual = as.numeric(over_test$CARAVAN)
predicted_3_num = as.numeric(predicted_3)
gain = gains(actual, predicted_3_num)

barplot(gain$mean.resp / mean(actual), names.arg = gain$depth, xlab = "Percentile", ylab = "Mean Response", main = "Decile-wise lift chart")

```


# Decision tree.
```{r}
fit1 <- rpart(formula=CARAVAN ~ .,data=over_train,control=rpart.control(minsplit=20, minbucket=1, cp=0.008))

fit1

glm_6 = glm(formula = CARAVAN ~  PPERSAUT+MOSTYPE+PBRAND+MBERHOOG+MBERMIDD, family = binomial(link = "logit"),
    data = over_train)


glm_6$xlevels[["PPERSAUT"]] <- union(glm_6$xlevels[["PPERSAUT"]], levels(over_test$PPERSAUT))
summary(glm_6)

predicted_6 = predict(glm_6, over_test, type = "response")

predictedClass_6 = ifelse(predicted_6>=0.5, 1, 0)


confusionMatrix(as.factor(predictedClass_6), as.factor(over_test$CARAVAN), positive = "1")

accuracy(predictedClass_6, as.numeric(over_test$CARAVAN))

pR2(glm_6)['McFadden']
# difference in deviance =  Null deviance (15177) - 12069 = 3108
```


# Model comparsion using anova test
```{r}
anova(logit.reg,model.2,model.3,glm_6,test = 'Chisq')
```


